# Каво и ШО
---
## Сайт

Сначала запускаем терраформ, разбил на несколько файлов для удобства работы с ними. Поднимается нужная нам инфроструктура в полностью завершенном виде. Убрал файлы пакетов из репозитория что бы место не занимали, терраформ и ансибл запускались с локальной виртуалки, но terraform из консоли, а с ансиблом тупо удобнее заботать из визуал студии. ***Большенство нюансов закоментил непосредственно в .tf файлах и в плейбуке ансибла, что бы объяснить те или иные решения.*** Из стороннего использовал коллекцию postgresql для удобного создания пользователя и базы. По мощьностям самих ВМ орентировался на минимальные требования из документации.

1. terraform apply:

![terraform apply](img/terraform%20apply.png)
---
А вот и результат:

![terraform complite](img/terraform%20complite.png)
---
Вот они все ВМ наглядно:

![все вм](/img/VM.png)
---
2. Создайте Target Group, включите в неё две созданных ВМ:
   
![Target Group](/img/target%20grup.png)
---
3. Создайте Backend Group, настройте backends на target group, ранее созданную. Настройте healthcheck на корень (/) и порт 80, протокол HTTP:
   
![Backend Group](/img/backend-group.png)
---
4. Создайте HTTP router. Путь укажите — /, backend group — созданную ранее:
   
![HTTP router](/img/http-router.png)
---
5. Создайте Application load balancer для распределения трафика на веб-сервера, созданные ранее. Укажите HTTP router, созданный ранее, задайте listener тип auto, порт 80:

![Application load balancer](/img/balanser.png)
---
6. Затем Ансибл (иначе на 80 порту глухо):

![ansible](/img/ansible.png)
---
Плейбук успешен с одного прогона, перезапустил т.к. опечатка была в роли:

![ansible complite](/img/ansible%20complite.png)
---

Тут добавлю пояснения касательно ансибла, первым делом сразу настроил проксирование через бастион и закинул приватную часть ключа в /root/.ssh/ для того что бы дальше плейбук играл без вопросов. Так же отключил gather_facts поскольку не пользуюсь ими, а как плюс получаем более шустрый плейбук. Основные задачи было решено разбить на роли таким образом финальный плейбук выходит компактным, а работать с отдельными ролями проще(исправлять ошибки, оптимизировать сам плейбук). И самое больное это репозиторий ELK, сколько мук, сначало с попытками использовать оригинальный, затем с зерколом яндекса, в итоге пришлось использовать предзагруженные пакеты и пушить их непосредственно в ВМ поскольку зерколо нестабильно и переодически ELK пропадало из него. Выбрал версию 7.14.1 поскольку кто то из однокурсников написал что она очень стабильная в общем чате, потом только убедился в этом посколько в 7 версии отсутствует xpac это сильно упрощает настройку и автоматизацию, не нужно генерировать пароли для эластика и кибаны. Так же очень хотелось бы избежать тупой замены конфигов, а вместо этого использовать модуль template но так я и не смог найти образец как должен выглядеть сам j2 шаблон для yml файла.

---
7. curl -v 158.160.10.83:80 (не вижу смысла отдельно добавлять курлы по каждому вебсерверу отдельно):

![curl](/img/curl%20-v.png)
---
Страница сайта:

![web-works](/img/web-works.png)
---
## Мониторинг

Все сконфигурированно ансиблом, нам остается только залогинится и настроить дашборды. Выбрал постгресс и апач, они проще настраиваются как мне показалось. Шаблон для агента Linux by Zabbix agent внем есть все исчерпывающие для нас метрики. Так же добавил агента на сам сервер просто что бы был.

Страница вебсервера после успешной настройки:

![zabbix-web-server](/img/zabbix-webserver-complite.png)
---
Страница с добавленными хостами, и зеленым статусом агента:

![zabbix-hosts](/img/zabbix-hosts.png)
---
Страница с настроенным дашбордом, по одному графу для CPU, RAM и диску(поигрался с контрастностью графиков что бы один от другого хорошо отличался) и отдельно по интерфейсам, поскольку графы очень информативные, будит месиво:

![zabbix-dashbord](/img/zabbix-dashbord.png)
---
## Логи

Ансибл делает все в исчерпывающем виде, рукми ничего делать не нужно, логи сразу идут с обоих web серверов, так же виден и ip kibana, на всякий приложил скрин из консоли разработчика где видна доступность elastic. logstash исключил из уровнения не то что бы он тут вообще нужен поскольку метрик не много.

Стрим логов:

![логи](/img/логи.png)
---
Это логи поступающие через модуль nginx:

![discover](/img/elc-discover.png)
---
Доступность эластика из консоли кибаны:

![elastic](/img/elastic.png)
---
## Сеть

Развернута одна VPC. Сервера web, Elasticsearch поместил в приватные подсети. Сервера Zabbix, Kibana, application load balancer поместил в публичную подсеть.

![сеть](/img/network.png)
---
Настройте Security Groups соответствующих сервисов на входящий трафик только к нужным портам. Дополнительно создал правило которое разрешает трафик между моими подсетями по всем портам и протаколам.

![sg](/img/secure-group.png)
---
Шлюз для доступа в сеть с ВМ которые не имеют NAT:

![gateway](/img/gateway.png)
---
Настройте ВМ с публичным адресом, в которой будет открыт только один порт — ssh. Настройте все security groups на разрешение входящего ssh из этой security group. Эта вм будет реализовывать концепцию bastion host. Потом можно будет подключаться по ssh ко всем хостам через этот хост.

![bastion](/img/bastion.png)

## Резервное копирование

Сделал задачу в шедулере для ежедневного копирования в 13:00:

![snapshot](/img/snapshot.png)
---
Готовые снапшоты:

![gotowo](/img/snapshot-complite.png)